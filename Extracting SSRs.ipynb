{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c83415-fd3d-4726-9dd2-4f21e2223938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import SeqIO\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f83a4-2bbf-4581-bfe0-8a1bac1412bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the ZIP file containing the genomes to be extracted\n",
    "zip_path = '/Path/to/downloaded/zipfiles.zip'\n",
    "\n",
    "# Directory where the extracted files will be placed\n",
    "extract_dir = 'extracted_genomes'\n",
    "\n",
    "# Check if the target extraction directory already exists\n",
    "if os.path.exists(extract_dir):\n",
    "    # If it exists, then delete it plus its contents for a clean extraction\n",
    "    shutil.rmtree(extract_dir)\n",
    "\n",
    "# Create a new extraction directory\n",
    "os.makedirs(extract_dir)\n",
    "\n",
    "# Open the ZIP file and extract its contents to the specified directory\n",
    "with zipfile.ZipFile(zip_path, 'r') as myzip:\n",
    "    myzip.extractall(extract_dir)\n",
    "\n",
    "# List all files and directories in the main extraction directory\n",
    "extracted_data = os.listdir(extract_dir)\n",
    "\n",
    "# Define the expected paths for specific subdirectories\n",
    "ncbi_dataset_dir = os.path.join(extract_dir, 'ncbi_dataset')  # Directory where the dataset is expected\n",
    "ncbi_data_dir = os.path.join(ncbi_dataset_dir, 'data')        # Subdirectory containing the main data\n",
    "\n",
    "# Check if the 'data' directory exists and is indeed a directory\n",
    "if os.path.exists(ncbi_data_dir) and os.path.isdir(ncbi_data_dir):\n",
    "    # If the directory exists, update the extracted data to reflect its contents\n",
    "    extracted_data = os.listdir(ncbi_data_dir)\n",
    "else:\n",
    "    # If the expected directory structure is missing, print a warning message\n",
    "    print(f\"Expected 'ncbi_dataset' directory not found in {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22d240-05ec-4758-843f-83151662f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data = []\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            json_file_path = os.path.join(root, file)\n",
    "            os.remove(json_file_path)\n",
    "\n",
    "# Create an empty list to store sequence data (used later on)\n",
    "sequence_data = []\n",
    "\n",
    "# Define the directory where processed data will be stored\n",
    "data_dir = 'data'\n",
    "\n",
    "# Create the `data` directory if it doesn't already exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Walk through all directories and files starting from the `extract_dir`\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    # Iterate over each file in the current directory\n",
    "    for file in files:\n",
    "        # Check if the file has a `.json` extension\n",
    "        if file.endswith(\".json\"):\n",
    "            # Get the full path to the JSON file\n",
    "            json_file_path = os.path.join(root, file)\n",
    "            # Remove the JSON file from the directory\n",
    "            os.remove(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e1008-371a-469d-82cc-e13bd71a435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each item (file or folder) in the `extracted_data` list\n",
    "for item in extracted_data:\n",
    "    # Create the full path of the item\n",
    "    item_path = os.path.join(ncbi_data_dir, item)\n",
    "    \n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(item_path):\n",
    "        # List all files in the current folder\n",
    "        files_in_folder = os.listdir(item_path)\n",
    "        \n",
    "        # Loop through each file in the folder\n",
    "        for filename in files_in_folder:\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(item_path, filename)\n",
    "            \n",
    "            # Check if the file is a nucleotide sequence file in FASTA format\n",
    "            if filename.endswith(\".fna\") or filename.endswith(\".fasta\"):\n",
    "                # Open the file for reading\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    # Parse the FASTA file and extract sequence records\n",
    "                    for record in SeqIO.parse(file, \"fasta\"):\n",
    "                        # Append the sequence ID and sequence to the `sequence_data` list\n",
    "                        sequence_data.append({\n",
    "                            'Sequence_ID': record.id,\n",
    "                            'Sequence': str(record.seq)\n",
    "                        })\n",
    "            \n",
    "            # Check if the file is a protein sequence file in FASTA format\n",
    "            elif filename.endswith(\".faa\"):\n",
    "                # Open the file for reading\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    # Parse the FASTA file and extract protein sequence records\n",
    "                    for record in SeqIO.parse(file, \"fasta\"):\n",
    "                        # Append the sequence ID and sequence to the `sequence_data` list\n",
    "                        sequence_data.append({\n",
    "                            'Sequence_ID': record.id,\n",
    "                            'Sequence': str(record.seq)\n",
    "                        })\n",
    "            \n",
    "            # Check if the file is a GenBank file\n",
    "            elif filename.endswith(\".gb\"):\n",
    "                # Open the file for reading\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    # Parse the GenBank file and extract the sequence record\n",
    "                    gb_record = SeqIO.read(file, \"genbank\")\n",
    "                    # Append the sequence ID and sequence to the `sequence_data` list\n",
    "                    sequence_data.append({\n",
    "                        'Sequence_ID': gb_record.id,\n",
    "                        'Sequence': str(gb_record.seq)\n",
    "                    })\n",
    "# If the `ncbi_dataset` directory is not found, print an error message\n",
    "else:\n",
    "    print(f\"Expected 'ncbi_dataset' directory not found in {ncbi_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8751718-f04f-4f5e-8b6f-1476179888d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column names for the dataframe\n",
    "# This step is to create a dataframe but a text file can be created if preferred\n",
    "columns = ['Sequence_ID', 'Sequence']\n",
    "\n",
    "# Create a pandas DataFrame from the `sequence_data` list with th columns 'Sequence_ID' and 'Sequence'\n",
    "genome_data = pd.DataFrame(sequence_data, columns=columns)\n",
    "\n",
    "# Define the path where the CSV file will be saved\n",
    "csv_path = 'Genome_Name.csv'\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "# The `index=False` argument ensures that the row index is not included in the CSV\n",
    "genome_data.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057e06c-b904-44c7-b451-e4fb60293a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_column = genome_data['Sequence']\n",
    "def smallest_rotation(ssr):\n",
    "    return min(ssr[i:] + ssr[:i] for i in range(len(ssr)))\n",
    "\n",
    "ssr_pattern = r'(\\w+?)\\1{4,50}'\n",
    "\n",
    "def process_chunk(sequence_chunk, start_pos, seen_positions):\n",
    "    chunk_ssrs = []\n",
    "\n",
    "    for match in re.finditer(ssr_pattern, sequence_chunk):\n",
    "        ssr = match.group(0)  # The matched SSR sequence\n",
    "        ssr_start = start_pos + match.start()  # Absolute position of the SSR start\n",
    "        ssr_end = ssr_start + len(ssr) - 1  # Absolute position of the SSR end\n",
    "        \n",
    "        if not any(start <= ssr_start <= end or start <= ssr_end <= end for start, end in seen_positions):\n",
    "            # Add the SSR and its position if no overlap\n",
    "            chunk_ssrs.append((ssr, ssr_start))\n",
    "            seen_positions.append((ssr_start, ssr_end))  # Track the SSR position\n",
    "\n",
    "    return chunk_ssrs\n",
    "\n",
    "# Extract the 'Sequence' column from the DataFrame\n",
    "sequence_column = genome_data['Sequence']\n",
    "\n",
    "# Define a function to compute the smallest rotation of a string\n",
    "def smallest_rotation(ssr):\n",
    "    # Generate all rotations of the string and return the smallest one to avoid counting same SSR more than once\n",
    "    return min(ssr[i:] + ssr[:i] for i in range(len(ssr)))\n",
    "\n",
    "# Define a regular expression pattern to match SSRs\n",
    "# This pattern looks for a repeating substring (\\w+?) repeated 4 to 50 times consecutively\n",
    "ssr_pattern = r'(\\w+?)\\1{4,50}'\n",
    "\n",
    "# Function to process a chunk of a sequence and extract unique SSRs\n",
    "# The chunk is to speed up the process however if computer is able to process large amounts of data this is not necessary\n",
    "def process_chunk(sequence_chunk, start_pos, seen_positions):\n",
    "    \n",
    "    # Create a list to store SSRs and their positions in the current chunk\n",
    "    chunk_ssrs = []\n",
    "\n",
    "    # Use regular expressions to find all matches for the SSR pattern in the sequence chunk\n",
    "    for match in re.finditer(ssr_pattern, sequence_chunk):\n",
    "        ssr = match.group(0)  # Extract the matched SSR sequence\n",
    "        ssr_start = start_pos + match.start()  # Calculate the absolute start position of the SSR\n",
    "        ssr_end = ssr_start + len(ssr) - 1  # Calculate the absolute end position of the SSR\n",
    "\n",
    "        # Check if this SSR overlaps with any previously seen SSR\n",
    "        if not any(start <= ssr_start <= end or start <= ssr_end <= end for start, end in seen_positions):\n",
    "            # If no overlap, add the SSR and its starting position to the results\n",
    "            chunk_ssrs.append((ssr, ssr_start))\n",
    "            # Record this SSR's position to track overlaps\n",
    "            seen_positions.append((ssr_start, ssr_end))\n",
    "\n",
    "    # Return the list of unique SSRs and their positions from this chunk\n",
    "    return chunk_ssrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d659a-7ecd-48ce-9758-228e2f30e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first genome sequence from the DataFrame's 'Sequence' column\n",
    "genome_sequence = genome_data['Sequence'].iloc[0]\n",
    "\n",
    "# Define the size of each sequence chunk to process and the step size for sliding the window\n",
    "chunk_size = 10000  # Number of bases in each chunk, adjust according to own circumstances\n",
    "step_size = 100    # Step size for sliding the window\n",
    "\n",
    "# Total number of sequences in the column (the total number of SSRs)\n",
    "total_sequences = len(sequence_column)\n",
    "\n",
    "# Total number of bases in the selected genome sequence (for calculating SSR density)\n",
    "total_bases = len(genome_sequence)\n",
    "\n",
    "# Initialize an empty dictionary to store unique SSRs and their positions\n",
    "unique_ssrs = {}\n",
    "\n",
    "# Define a function to find SSRs using a sliding window approach\n",
    "def sliding_window_ssr_finder(genome_sequence, chunk_size, step_size):\n",
    "\n",
    "    total_bases = len(genome_sequence)  # Total length of the genome sequence\n",
    "    seen_positions = []  # List to track positions of already processed SSRs\n",
    "\n",
    "    # Iterate over the genome sequence using a sliding window approach\n",
    "    for start in tqdm(range(0, total_bases - chunk_size + 1, step_size), desc=\"Finding SSRs\"):\n",
    "        # Extract a chunk of the sequence based on the current window\n",
    "        sequence_chunk = genome_sequence[start:start + chunk_size]\n",
    "\n",
    "        # Process the chunk to find SSRs and their positions\n",
    "        chunk_ssrs = process_chunk(sequence_chunk, start, seen_positions)\n",
    "\n",
    "        # Store unique SSRs in the dictionary\n",
    "        for ssr, pos in chunk_ssrs:\n",
    "            if ssr not in unique_ssrs:\n",
    "                # Add the SSR to the dictionary with its first occurrence position\n",
    "                unique_ssrs[ssr] = pos\n",
    "\n",
    "    # Return the dictionary of unique SSRs\n",
    "    return unique_ssrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9174ba4-979e-4b6f-a2bd-27ddfc3b8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the `sliding_window_ssr_finder` function to find unique SSRs in the genome sequence\n",
    "# Produces loading bar to indicate progress\n",
    "# genome_sequence: The entire genome sequence being analyzed\n",
    "# chunk_size: The size of each sliding window or chunk to process\n",
    "# step_size: The step size for the sliding window movement\n",
    "unique_ssrs = sliding_window_ssr_finder(genome_sequence, chunk_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b1f67-f05b-44ae-b3a5-c9d8d13e4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a text file in write mode to save the identified SSRs and their positions\n",
    "# Replace 'Genome' with the actual genome\n",
    "with open(\"Genome_ssrs_found.txt\", \"w\") as file:\n",
    "    # Iterate over each SSR and its position in the `unique_ssrs` dictionary\n",
    "    for ssr, pos in unique_ssrs.items():\n",
    "        # Write the SSR and its position to the file in a formatted string\n",
    "        file.write(f\"SSR: {ssr}, Position: {pos}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd97682-4c77-4b07-a613-07a03df689dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of bases in kilobases (kbp)\n",
    "total_bases_kbp = total_bases / 1000  # Convert total bases from base pairs to kilobases\n",
    "\n",
    "# Calculate the total number of unique SSRs identified\n",
    "total_ssrs = len(unique_ssrs)\n",
    "\n",
    "# Join all unique SSR sequences into a single string for calculating total SSR bases\n",
    "ssr_string = ''.join(unique_ssrs)\n",
    "\n",
    "# Calculate the total number of bases covered by all SSRs\n",
    "total_ssr_bases = len(ssr_string)\n",
    "\n",
    "# Calculate SSR density (total SSR bases per kilobase of genome sequence)\n",
    "ssr_density = total_ssr_bases / total_bases_kbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aef66a-700a-4390-8f9d-6807a2f73d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the genome has any bases (total_bases > 0) before calculating percentages\n",
    "if total_bases > 0:\n",
    "\n",
    "    # Count the number of occurrences of each nucleotide (A, T, C, G) in the genome sequence\n",
    "    count_A = genome_sequence.count('A')  # Count the number of 'A's\n",
    "    count_T = genome_sequence.count('T')  # Count the number of 'T's\n",
    "    count_C = genome_sequence.count('C')  # Count the number of 'C's\n",
    "    count_G = genome_sequence.count('G')  # Count the number of 'G's\n",
    "    \n",
    "    # Calculate the percentage\n",
    "    percentage_A = (count_A / total_bases) * 100  # Percentage of 'A'\n",
    "    percentage_T = (count_T / total_bases) * 100  # Percentage of 'T'\n",
    "    percentage_C = (count_C / total_bases) * 100  # Percentage of 'C'\n",
    "    percentage_G = (count_G / total_bases) * 100  # Percentage of 'G'\n",
    "    \n",
    "    # Optional:\n",
    "    # Print the percentages of each nucleotide (two decimal places)\n",
    "    print(f\"Percentage of A: {percentage_A: .2f}%\")\n",
    "    print(f\"Percentage of T: {percentage_T: .2f}%\")\n",
    "    print(f\"Percentage of C: {percentage_C: .2f}%\")\n",
    "    print(f\"Percentage of G: {percentage_G: .2f}%\")\n",
    "else:\n",
    "    # If no sequence data exists (total_bases <= 0), print a message\n",
    "    print(\"No SSRs were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546cd19-d573-4a87-9d63-2755a742f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any SSR bases to analyze (total_ssr_bases > 0)\n",
    "if total_ssr_bases > 0:\n",
    "\n",
    "    # Count the occurrences of each nucleotide (A, T, C, G) in the compiled SSR string\n",
    "    ssr_count_A = ssr_string.count('A')  # Count the number of 'A's in the SSRs\n",
    "    ssr_count_T = ssr_string.count('T')  # Count the number of 'T's in the SSRs\n",
    "    ssr_count_C = ssr_string.count('C')  # Count the number of 'C's in the SSRs\n",
    "    ssr_count_G = ssr_string.count('G')  # Count the number of 'G's in the SSRs\n",
    "    \n",
    "    # Calculate the percentage of each nucleotide in the SSR sequences\n",
    "    ssr_percentage_A = (ssr_count_A / total_ssr_bases) * 100  # Percentage of 'A' in SSRs\n",
    "    ssr_percentage_T = (ssr_count_T / total_ssr_bases) * 100  # Percentage of 'T' in SSRs\n",
    "    ssr_percentage_C = (ssr_count_C / total_ssr_bases) * 100  # Percentage of 'C' in SSRs\n",
    "    ssr_percentage_G = (ssr_count_G / total_ssr_bases) * 100  # Percentage of 'G' in SSRs\n",
    "    \n",
    "    # Optional:\n",
    "    # Print the percentages of A, T, C, and G in the SSR sequences (rounded to two decimal places)\n",
    "    print(f\"Percentage of A: {ssr_percentage_A: .2f}%\")\n",
    "    print(f\"Percentage of T: {ssr_percentage_T: .2f}%\")\n",
    "    print(f\"Percentage of C: {ssr_percentage_C: .2f}%\")\n",
    "    print(f\"Percentage of G: {ssr_percentage_G: .2f}%\")\n",
    "else:\n",
    "    # If no SSRs were found (total_ssr_bases <= 0), print a message\n",
    "    print(\"No SSRs were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b54490-7d6e-4b79-a945-6561f39ecf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Genome \n",
    "Genome = 'Name of Genome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d22aa-7ff7-4e47-be91-8f197a909a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the CSV where data will be saved\n",
    "csv_path = \"/path/to/saved/data.csv\"\n",
    "\n",
    "# Create a dictionary representing a new row of data to add to the CSV file\n",
    "new_row = {\n",
    "    \n",
    "    \"Genome\": Genome,  # The genome name/ID\n",
    "\n",
    "    \"total_bases\": total_bases,  # The total length of the genome sequence (in base pairs)\n",
    "\n",
    "    \"total_ssrs\": total_ssrs,  # The count of unique SSR sequences identified\n",
    "\n",
    "    \"total_ssr_bases\": total_ssr_bases,  # Total number of bases in the SSR sequences\n",
    "\n",
    "    \"ssr_density\": ssr_density,  # The density of SSRs in the genome sequence (bases/kb)\n",
    "\n",
    "    # The percentages of each nucleotide (A, T, C, G) in the genome sequence\n",
    "    \"%A in bases\": percentage_A, \n",
    "    \"%T in bases\": percentage_T,\n",
    "    \"%C in bases\": percentage_C, \n",
    "    \"%G in bases\": percentage_G, \n",
    "\n",
    "    # The percentages of each nucleotide (A, T, C, G) in the SSR sequences\n",
    "    \"%A in ssrs\": ssr_percentage_A, \n",
    "    \"%T in ssrs\": ssr_percentage_T, \n",
    "    \"%C in ssrs\": ssr_percentage_C, \n",
    "    \"%G in ssrs\": ssr_percentage_G \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851aea8-d948-4d90-b502-2e93941af629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the existing CSV file into a DataFrame (df)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert the new_row dictionary into a DataFrame so it can be added to the existing DataFrame\n",
    "new_row_df = pd.DataFrame([new_row])  \n",
    "\n",
    "# Add the new row DataFrame with the existing DataFrame (df), ignoring the index to avoid conflicts\n",
    "df = pd.concat([df, new_row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691a611-0463-4bad-9bb3-090a36411da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the updated DataFrame (df) to the CSV file\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfc08a-7dc5-499d-812b-d5d42e4d5589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
